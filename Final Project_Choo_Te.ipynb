{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "500f339c-765b-47d2-b277-05ab611d68f0"
    }
   },
   "source": [
    "# Thumbs Up? Sentiment Classification using Machine Learning Techniques\n",
    "originally by: Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
    "\n",
    "re-created by: Yel Choo and Robee Te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f080a828-69e1-409b-8ffe-5d34906c3958"
    }
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "80cf5e68-5464-4c6b-bcef-1edf8cc5367e"
    }
   },
   "source": [
    "To start, we first downloaded the dataset found in http://www.cs.cornell.edu/people/pabo/movie-review-data/. We used the polarity version 0.9 which was the dataset used by the orginal authors to derived at the same results later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9991b1c1-074d-46e5-a2f4-0e1eb4cd7679"
    }
   },
   "source": [
    "## Reading the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d92c3467-8dc9-4ad4-bf5b-86aee5bcdc85"
    }
   },
   "source": [
    "To perform the actual classification, we must first read the data. The ReadFile.py file, contains 3 methods, namely, readFile(), getCorpusNeg(), and getCorpusPos().\n",
    "\n",
    "The readFile() method just iterates through the whole dataset separating the negative documents from the positive documents. The getCorpusNeg() and getCorpusPos() returns the positive and negative documents, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9629cb93-c6ff-4a87-9946-fbac42755c2c"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "class ReadFile(object):\n",
    "    def __init__(self):\n",
    "        self.corpus = []\n",
    "        self.corpus_neg = []\n",
    "        self.corpus_pos = []\n",
    "        \n",
    "    def readFile(self):\n",
    "            path = \"C:/Users/Robee Khyra Te/Documents/GitHub/machlrn/Final Project/tokens/*\"\n",
    "\n",
    "            for folder in glob.glob(path):\n",
    "                if \"neg\" in folder:\n",
    "                    for textfile in glob.glob(folder.replace(\"\\\\neg\", \"/neg/*\")):\n",
    "                        with open(textfile.replace(\"\\\\\", \"/\")) as f:\n",
    "                            text = f.read()\n",
    "                        self.corpus_neg.append((text, 0))\n",
    "                elif \"pos\" in folder:\n",
    "                    for textfile in glob.glob(folder.replace(\"\\\\pos\", \"/pos/*\")):\n",
    "                        with open(textfile.replace(\"\\\\\", \"/\")) as f:\n",
    "                            text = f.read()\n",
    "                        self.corpus_pos.append((text, 1))\n",
    "\n",
    "            print(len(self.corpus_neg))\n",
    "            print(len(self.corpus_pos))\n",
    "    \n",
    "    def getCorpusNeg(self):\n",
    "        return self.corpus_neg\n",
    "\n",
    "    def getCorpusPos(self):\n",
    "        return self.corpus_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d0e86ea8-d2f1-4300-8f96-0f0a6d644a87"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "rf = ReadFile()\n",
    "rf.readFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "d5b8f58a-b103-4eca-b4d5-2fc42b3d63b7"
    }
   },
   "source": [
    "After reading the file, we must pre-process each of them. The original authors adapted the technique used by Das and Chen (2001) in which they added a string \"NOT_\" to every word after the negation words (no, isn't, wasn't, among others) and the first punctuation mark.\n",
    "\n",
    "In our case, the negation words used were no, not, and every word ending in n't. We can easily find the negation words because nltk's word tokenizer automatically splits the word didn't to [did and n't]. While, for the punctuation marks we used [, . ? ! ;]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7e92641d-f1f6-4e9a-aada-0cf177a0dd6e"
    }
   },
   "source": [
    "To be able to do this, we had a class named Features, the method getListWithNegation() will be the one appendning \"NOT_\" to the given documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "27b4b8ed-eb54-462e-b9d7-4af2c26dff91"
    }
   },
   "outputs": [],
   "source": [
    "def getListWithNegation(self, corpus):\n",
    "    all_document = []\n",
    "\n",
    "    for document, category in corpus:\n",
    "        all_words = []\n",
    "        tokens = nltk.word_tokenize(document)\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if tokens[i] in self.negation_list:\n",
    "                all_words.append(tokens[i])\n",
    "                i += 1\n",
    "                while i < len(tokens):\n",
    "                    if tokens[i] not in self.punctuation_list:\n",
    "                        a = tokens[i] + self.negate\n",
    "                        all_words.append(a)\n",
    "                    else:\n",
    "                        break\n",
    "                    i += 1\n",
    "            else:\n",
    "                all_words.append(tokens[i])\n",
    "            i += 1\n",
    "\n",
    "        all_document.append((all_words, category))\n",
    "\n",
    "    #print(all_document)\n",
    "\n",
    "    #print(all_words)\n",
    "\n",
    "    return all_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8026867b-8727-4a8f-b7d5-f950824c4be0"
    }
   },
   "source": [
    "After the pre-processing of the dataset, we are now ready to perform the actual classification of sentiments to the dataset. There are 8 different combination that must be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "88086832-0cbf-4a88-912f-43a9d0669f6e"
    }
   },
   "source": [
    "### (1) Unigrams using Frequency\n",
    "First, we must chose the features to be used. In the original document, they only choose the word that appears 4 or more times. A method was created to count the number of occurrences of each word in the documents and filter the words which appears 4 or more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getUnigram(self, all_document):\n",
    "        unigrams = {}\n",
    "\n",
    "        for words_in_document, category in all_document:\n",
    "            for word in words_in_document:\n",
    "                unigrams[word] = unigrams.get(word, 0) + 1\n",
    "\n",
    "        for word, count in unigrams.items():\n",
    "            if count >= 4: #SABI SA DOCU AT LEAST 4\n",
    "                self.features['unigram'].append(word)\n",
    "\n",
    "        #print(len(self.features['unigram']))\n",
    "\n",
    "        return self.features['unigram']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the words, the next step is to get the frequency of the chosen unigrams which will be performed by the getChosenFeatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getChosenFeatures(self, all_document, type = 'None'):\n",
    "        chosen_features = np.zeros((len(all_document), len(self.features['unigram'])))\n",
    "\n",
    "        i = 0\n",
    "        for words_in_document, category in all_document:\n",
    "            if type == 'frequency':\n",
    "                frequencies = Counter(words_in_document)\n",
    "                for word, count in frequencies.items():\n",
    "                    try:\n",
    "                        chosen_features[i][self.features['unigram'].index(word)] = count\n",
    "                    except: #PAG WALA\n",
    "                        pass\n",
    "            elif type == 'presence':\n",
    "                for word in set(words_in_document):\n",
    "                    try:\n",
    "                        chosen_features[i][self.features['unigram'].index(word)] = 1\n",
    "                    except: #PAG WALA\n",
    "                        pass\n",
    "            i += 1\n",
    "\n",
    "        #print(chosen_features)\n",
    "\n",
    "        return chosen_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to split the features into n folds, which will then be used to get the training dataset and the testing dataset will finally be passed to the classifiers. \n",
    "\n",
    "The Classify class contains 3 methods: splitToMiniBatches, naiveBayes Classifier, and SVMClassifier. The splitToMiniBatches method will just split the whole datset into 3 batches, then for each batch it will split the features into the train and test array given the indexOfTrain. \n",
    "\n",
    "The paper stated that we must maintain a balanced distribution between the 2 classes (negative and positive) that's why the number of negative features present in each batch['x'] must be equal to the number of positive features. For the batch['y'], we just assign 0 - if the feature is from the negative documents and 1 - if it is from the positive documents.\n",
    "\n",
    "To split the dataset equally, we used scikit-learn's KFold function (ref: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitToMiniBatches(self, negative, positive, indexOfTest):\n",
    "    batch = {'fold' : [], 'X' : [], 'Y' : []}\n",
    "\n",
    "    split = len(negative) // self.number_fold\n",
    "\n",
    "    #Create mini batches\n",
    "    for i in range(self.number_fold):\n",
    "        mergedlist = []\n",
    "\n",
    "        batch['fold'].append(i)\n",
    "\n",
    "        #Split to equal sized batch, maintaining balance from neg and pos\n",
    "        neg = negative[i * split : i * split + split]\n",
    "        #print(len(neg))\n",
    "        pos = positive[i * split : i * split + split]\n",
    "        #print(len(pos))\n",
    "\n",
    "        mergedlist.extend(neg)\n",
    "        mergedlist.extend(pos)\n",
    "        batch['X'].append(mergedlist)\n",
    "        #print(len(self.batch['X'][i]))\n",
    "\n",
    "        batch['Y'].append(np.append(np.zeros(split), np.ones(split)))\n",
    "        #print(len(self.batch['Y'][i]))\n",
    "\n",
    "    batch['fold'] = np.array(batch['fold'])\n",
    "    batch['X'] = np.array(batch['X'])\n",
    "    batch['Y'] = np.array(batch['Y'])\n",
    "\n",
    "    x_train = batch['X'][batch['fold'] != i]\n",
    "    y_train = batch['Y'][batch['fold'] != i]\n",
    "\n",
    "    x_test = batch['X'][i]\n",
    "    y_test = batch['Y'][i]\n",
    "\n",
    "    #print(x_test.shape)\n",
    "    #print(y_test.shape)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual testing, we used scikit-learns's Naive Bayes classifier for multivariate Bernoulli models (ref: http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html). \n",
    "And scikit-learn's Linear Support Vector Classifier (ref: http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naiveBayesClassifier(self, x_train, x_test, y_train, y_test):\n",
    "    bernoulliNB = BernoulliNB()\n",
    "    bernoulliNB.fit(x_train.reshape(x_train.shape[0] * x_train.shape[1], -1), np.ravel(y_train.reshape(y_train.shape[0] * y_train.shape[1], -1)))\n",
    "    accuracy = accuracy_score(bernoulliNB.predict(x_test), y_test)\n",
    "\n",
    "    #print(\"Accuracy for 1 fold in Naive Bayes: \", accuracy)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def SVMClassifier(self, x_train, x_test, y_train, y_test):\n",
    "    linearSVM = LinearSVC()\n",
    "    linearSVM.fit(x_train.reshape(x_train.shape[0] * x_train.shape[1], -1), np.ravel(y_train.reshape(y_train.shape[0] * y_train.shape[1], -1)))\n",
    "    accuracy = accuracy_score(linearSVM.predict(x_test), y_test)\n",
    "\n",
    "    #print(\"Accuracy for 1 fold in SVM: \", accuracy)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will test this using 2 classifiers: Naive Bayes and SVM. As stated in the paper, we will run this 3 times and get the average of the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7472\n",
      "15690\n",
      "Accuracy for 1 fold in Naive Bayes:  0.787096774194\n",
      "Accuracy for 1 fold in SVM:  0.783870967742\n",
      "7289\n",
      "15386\n",
      "Accuracy for 1 fold in Naive Bayes:  0.796774193548\n",
      "Accuracy for 1 fold in SVM:  0.777419354839\n",
      "7416\n",
      "15575\n",
      "Accuracy for 1 fold in Naive Bayes:  0.78064516129\n",
      "Accuracy for 1 fold in SVM:  0.767741935484\n",
      "Average Accuracy of the Naive Bayes Classifier:  78.8172043011\n",
      "Average Accuracy of the SVM Classifier:  77.6344086022\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "f = Features()\n",
    "negativeDocuments = f.getListWithNegation(rf.getCorpusNeg())\n",
    "positiveDocuments = f.getListWithNegation(rf.getCorpusPos())\n",
    "\n",
    "kf = KFold(len(rf.getCorpusNeg()))\n",
    "\n",
    "aveAccuracyNB = 0\n",
    "aveAccuracySVM = 0\n",
    "indexOfTest = 0\n",
    "\n",
    "for train, test in kf: #Get training indices and testing indices\n",
    "    negDoc = []\n",
    "    posDoc = []\n",
    "\n",
    "    for index in train:\n",
    "        negDoc.append(negativeDocuments[index])\n",
    "        posDoc.append(positiveDocuments[index])\n",
    "\n",
    "    negativeUnigrams = f.getUnigram(negDoc)\n",
    "    positiveUnigrams = f.getUnigram(posDoc)\n",
    "\n",
    "    negativeFeatures = f.getChosenFeatures(negDoc, type = 'frequency')\n",
    "    positiveFeatures = f.getChosenFeatures(posDoc, type = 'frequency')\n",
    "\n",
    "    f.features['unigram'] = []\n",
    "\n",
    "    c = Classify()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = c.splitToMiniBatches(negativeFeatures, positiveFeatures, indexOfTest)\n",
    "\n",
    "    aveAccuracyNB += c.naiveBayesClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    aveAccuracySVM += c.SVMClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    indexOfTest += 1\n",
    "\n",
    "print(\"Average Accuracy of the Naive Bayes Classifier: \", aveAccuracyNB / 3 * 100)\n",
    "print(\"Average Accuracy of the SVM Classifier: \", aveAccuracySVM / 3 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "66323629-cdbf-4f78-86ad-6fbb54c37c55"
    }
   },
   "source": [
    "### (2) Unigrams using Presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of getting the features was the same. The only difference was that instead of frequency, we will just check if the feature is present in the document or not. 1 if it is present and 0 otherwise. The testing process was also similar to the Unigrams using frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7472\n",
      "15690\n",
      "Accuracy for 1 fold in Naive Bayes:  0.787096774194\n",
      "Accuracy for 1 fold in SVM:  0.758064516129\n",
      "7289\n",
      "15386\n",
      "Accuracy for 1 fold in Naive Bayes:  0.796774193548\n",
      "Accuracy for 1 fold in SVM:  0.81935483871\n",
      "7416\n",
      "15575\n",
      "Accuracy for 1 fold in Naive Bayes:  0.78064516129\n",
      "Accuracy for 1 fold in SVM:  0.803225806452\n",
      "Average Accuracy of the Naive Bayes Classifier: \n",
      "78.8172043011\n",
      "Average Accuracy of the SVM Classifier: \n",
      "79.3548387097\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "f = Features()\n",
    "negativeDocuments = f.getListWithNegation(rf.getCorpusNeg())\n",
    "positiveDocuments = f.getListWithNegation(rf.getCorpusPos())\n",
    "\n",
    "kf = KFold(len(rf.getCorpusNeg()))\n",
    "\n",
    "aveAccuracyNB = 0\n",
    "aveAccuracySVM = 0\n",
    "indexOfTest = 0\n",
    "\n",
    "for train, test in kf: #Get training indices and testing indices\n",
    "    negDoc = []\n",
    "    posDoc = []\n",
    "\n",
    "    for index in train:\n",
    "        negDoc.append(negativeDocuments[index])\n",
    "        posDoc.append(positiveDocuments[index])\n",
    "\n",
    "    negativeUnigrams = f.getUnigram(negDoc)\n",
    "    positiveUnigrams = f.getUnigram(posDoc)\n",
    "\n",
    "    negativeFeatures = f.getChosenFeatures(negDoc, type = 'presence')\n",
    "    positiveFeatures = f.getChosenFeatures(posDoc, type = 'presence')\n",
    "\n",
    "    f.features['unigram'] = []\n",
    "\n",
    "    c = Classify()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = c.splitToMiniBatches(negativeFeatures, positiveFeatures, indexOfTest)\n",
    "\n",
    "    aveAccuracyNB += c.naiveBayesClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    aveAccuracySVM += c.SVMClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    indexOfTest += 1\n",
    "\n",
    "print(\"Average Accuracy of the Naive Bayes Classifier: \")\n",
    "print(aveAccuracyNB / 3 * 100)\n",
    "print(\"Average Accuracy of the SVM Classifier: \")\n",
    "print(aveAccuracySVM / 3 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b3cb178b-5168-4df2-854c-0094313d1888"
    }
   },
   "source": [
    "### (3) Bigrams using Presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this combination instead of just getting the unigram, we will now use bigram. For us to get the bigrams, we used nltk's ngrams function and passed the tokenized words in the document and it returns all the bigrams from the documents (e.g. (word1, word2)). In the paper, it is stated that they choose the bigrams which appeared at least 7 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBigram(self, all_document):\n",
    "    bigrams = {}\n",
    "    words = []\n",
    "\n",
    "    for document, category in all_document:\n",
    "        tokens = nltk.word_tokenize(document)\n",
    "        words += ngrams(tokens, 2)\n",
    "\n",
    "    for word in words:\n",
    "        bigrams[word] = bigrams.get(word, 0) + 1\n",
    "\n",
    "    for word, count in bigrams.items():\n",
    "        if count >= 7: #SABI SA DOCU AT LEAST 7\n",
    "            self.features['bigram'].append(word)\n",
    "\n",
    "    #print(self.features['bigram'][:100])\n",
    "    return self.features['bigram'][:16165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 fold in Naive Bayes:  0.793548387097\n",
      "Accuracy for 1 fold in SVM:  0.777419354839\n",
      "Accuracy for 1 fold in Naive Bayes:  0.764516129032\n",
      "Accuracy for 1 fold in SVM:  0.764516129032\n",
      "Accuracy for 1 fold in Naive Bayes:  0.741935483871\n",
      "Accuracy for 1 fold in SVM:  0.758064516129\n",
      "Average Accuracy of the Naive Bayes Classifier: \n",
      "76.6666666667\n",
      "Average Accuracy of the SVM Classifier: \n",
      "76.6666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "f = Features()\n",
    "negativeDocuments = f.getListWithNegation(rf.getCorpusNeg())\n",
    "positiveDocuments = f.getListWithNegation(rf.getCorpusPos())\n",
    "\n",
    "kf = KFold(len(rf.getCorpusNeg()))\n",
    "\n",
    "aveAccuracyNB = 0\n",
    "aveAccuracySVM = 0\n",
    "indexOfTest = 0\n",
    "\n",
    "for train, test in kf: #Get training indices and testing indices\n",
    "    negDoc = []\n",
    "    posDoc = []\n",
    "\n",
    "    for index in train:\n",
    "        negDoc.append(negativeDocuments[index])\n",
    "        posDoc.append(positiveDocuments[index])\n",
    "\n",
    "    negativeUnigrams = f.getBigram(negDoc)\n",
    "    positiveUnigrams = f.getBigram(posDoc)\n",
    "\n",
    "    negativeFeatures = f.getChosenFeaturesBigram(negDoc)\n",
    "    positiveFeatures = f.getChosenFeaturesBigram(posDoc)\n",
    "\n",
    "    f.features['bigram'] = []\n",
    "\n",
    "    c = Classify()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = c.splitToMiniBatches(negativeFeatures, positiveFeatures, indexOfTest)\n",
    "\n",
    "    aveAccuracyNB += c.naiveBayesClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    aveAccuracySVM += c.SVMClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    indexOfTest += 1\n",
    "\n",
    "print(\"Average Accuracy of the Naive Bayes Classifier: \")\n",
    "print(aveAccuracyNB / 3 * 100)\n",
    "print(\"Average Accuracy of the SVM Classifier: \")\n",
    "print(aveAccuracySVM / 3 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cf85f393-c27d-4b7e-9d86-7cc04d02cfce"
    }
   },
   "source": [
    "### (4) Unigrams + Bigrams using Presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just combines the features from the Unigram and Bigram chosen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(1240, 0)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-2080761e0449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitToMiniBatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmergedlistNegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmergedlistPositive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexOfTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0maveAccuracyNB\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaiveBayesClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0maveAccuracySVM\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVMClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-75f9b791fee5>\u001b[0m in \u001b[0;36mnaiveBayesClassifier\u001b[0;34m(self, x_train, x_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnaiveBayesClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mbernoulliNB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mbernoulliNB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbernoulliNB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Robee Khyra Te\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \"\"\"\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Robee Khyra Te\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    508\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    509\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32mC:\\Users\\Robee Khyra Te\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    413\u001b[0m                              \u001b[1;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                              % (n_features, shape_repr, ensure_min_features,\n\u001b[0;32m--> 415\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype_orig\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(1240, 0)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "f = Features()\n",
    "negativeDocuments = f.getListWithNegation(rf.getCorpusNeg())\n",
    "positiveDocuments = f.getListWithNegation(rf.getCorpusPos())\n",
    "\n",
    "kf = KFold(len(rf.getCorpusNeg()))\n",
    "\n",
    "aveAccuracyNB = 0\n",
    "aveAccuracySVM = 0\n",
    "indexOfTest = 0\n",
    "\n",
    "for train, test in kf: #Get training indices and testing indices\n",
    "    negDoc = []\n",
    "    posDoc = []\n",
    "\n",
    "    for index in train:\n",
    "        negDoc.append(negativeDocuments[index])\n",
    "        posDoc.append(positiveDocuments[index])\n",
    "    \n",
    "    mergedlistNegative = []\n",
    "    mergedlistNegative.extend(f.getChosenFeatures(negDoc, type = 'presence'))\n",
    "    mergedlistNegative.extend(f.getChosenFeaturesBigram(negDoc))\n",
    "    \n",
    "    mergedlistPositive = []\n",
    "    mergedlistPositive.extend(f.getChosenFeatures(posDoc, type = 'presence'))\n",
    "    mergedlistPositive.extend(f.getChosenFeaturesBigram(posDoc))\n",
    "\n",
    "    #negativeFeatures = f.getChosenFeaturesUniBigram(negDoc, mergedlistNegative)\n",
    "    #positiveFeatures = f.getChosenFeaturesUniBigram(posDoc, mergedlistPositive)\n",
    "\n",
    "    f.features['unigram_bigram'] = []\n",
    "\n",
    "    c = Classify()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = c.splitToMiniBatches(mergedlistNegative, mergedlistPositive, indexOfTest)\n",
    "\n",
    "    aveAccuracyNB += c.naiveBayesClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    aveAccuracySVM += c.SVMClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    indexOfTest += 1\n",
    "\n",
    "print(\"Average Accuracy of the Naive Bayes Classifier: \")\n",
    "print(aveAccuracyNB / 3 * 100)\n",
    "print(\"Average Accuracy of the SVM Classifier: \")\n",
    "print(aveAccuracySVM / 3 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "812e378d-9869-4cf6-9129-b54efa80fb3b"
    }
   },
   "source": [
    "### (5) Top Unigrams using Presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of getting the features was the same. The only difference of this from the original unigrams was that instead of all the unigrams, we will just get the top 2,633 unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTopUnigrams(self, all_document):\n",
    "        frequencies = Counter(self.getUnigram(all_document))\n",
    "        self.features['topunigrams'] = sorted(frequencies, key=frequencies.get, reverse=True)[:2633]\n",
    "\n",
    "        chosen_features = np.zeros((len(all_document), len(self.features['topunigrams'])))\n",
    "\n",
    "        i = 0\n",
    "        for words_in_document, category in all_document:\n",
    "            for word in set(words_in_document):\n",
    "                try:\n",
    "                    chosen_features[i][self.features['topunigrams'].index(word)] = 1\n",
    "                except: #PAG WALA\n",
    "                    pass\n",
    "            i += 1\n",
    "\n",
    "        #print(chosen_features)\n",
    "        return chosen_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 fold in Naive Bayes:  1.0\n",
      "Accuracy for 1 fold in SVM:  1.0\n",
      "Accuracy for 1 fold in Naive Bayes:  1.0\n",
      "Accuracy for 1 fold in SVM:  1.0\n",
      "Accuracy for 1 fold in Naive Bayes:  1.0\n",
      "Accuracy for 1 fold in SVM:  1.0\n",
      "Average Accuracy of the Naive Bayes Classifier: \n",
      "100.0\n",
      "Average Accuracy of the SVM Classifier: \n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "f = Features()\n",
    "negativeDocuments = f.getListWithNegation(rf.getCorpusNeg())\n",
    "positiveDocuments = f.getListWithNegation(rf.getCorpusPos())\n",
    "\n",
    "kf = KFold(len(rf.getCorpusNeg()))\n",
    "\n",
    "aveAccuracyNB = 0\n",
    "aveAccuracySVM = 0\n",
    "indexOfTest = 0\n",
    "\n",
    "for train, test in kf: #Get training indices and testing indices\n",
    "    negDoc = []\n",
    "    posDoc = []\n",
    "\n",
    "    for index in train:\n",
    "        negDoc.append(negativeDocuments[index])\n",
    "        posDoc.append(positiveDocuments[index])\n",
    "\n",
    "    negativeFeatures = f.getTopUnigrams(negDoc)\n",
    "    positiveFeatures = f.getTopUnigrams(posDoc)\n",
    "\n",
    "    f.features['topunigrams'] = []\n",
    "\n",
    "    c = Classify()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = c.splitToMiniBatches(negativeFeatures, positiveFeatures, indexOfTest)\n",
    "\n",
    "    aveAccuracyNB += c.naiveBayesClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    aveAccuracySVM += c.SVMClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    indexOfTest += 1\n",
    "\n",
    "print(\"Average Accuracy of the Naive Bayes Classifier: \")\n",
    "print(aveAccuracyNB / 3 * 100)\n",
    "print(\"Average Accuracy of the SVM Classifier: \")\n",
    "print(aveAccuracySVM / 3 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "51f497b7-309c-4f27-a06d-9dfd3699b6d6"
    }
   },
   "source": [
    "### (6) Unigrams + POS tags using Presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also has the process as the original unigram but we first need to determine the POS tag of the words. To be able to do this, we utilized nltk's POS tagger. Each POS tag was append at the end of each word (e.g. Hard to Hard-ADJ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 fold in Naive Bayes:  0.777419354839\n",
      "Accuracy for 1 fold in SVM:  0.761290322581\n",
      "Accuracy for 1 fold in Naive Bayes:  0.777419354839\n",
      "Accuracy for 1 fold in SVM:  0.790322580645\n",
      "Accuracy for 1 fold in Naive Bayes:  0.770967741935\n",
      "Accuracy for 1 fold in SVM:  0.806451612903\n",
      "Average Accuracy of the Naive Bayes Classifier: \n",
      "77.5268817204\n",
      "Average Accuracy of the SVM Classifier: \n",
      "78.6021505376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "f = Features()\n",
    "negativeDocuments = f.getListWithNegation(rf.getCorpusNeg())\n",
    "positiveDocuments = f.getListWithNegation(rf.getCorpusPos())\n",
    "\n",
    "kf = KFold(len(rf.getCorpusNeg()))\n",
    "\n",
    "aveAccuracyNB = 0\n",
    "aveAccuracySVM = 0\n",
    "indexOfTest = 0\n",
    "\n",
    "for train, test in kf: #Get training indices and testing indices\n",
    "    negDoc = []\n",
    "    posDoc = []\n",
    "\n",
    "    for index in train:\n",
    "        negDoc.append(negativeDocuments[index])\n",
    "        posDoc.append(positiveDocuments[index])\n",
    "\n",
    "    negativeUnigrams = f.getUnigramPos(negDoc)\n",
    "    positiveUnigrams = f.getUnigramPos(posDoc)\n",
    "    \n",
    "    negativeFeatures = f.getChosenFeaturesPos(negDoc)\n",
    "    positiveFeatures = f.getChosenFeaturesPos(posDoc)\n",
    "\n",
    "    f.features['unigrams_pos'] = []\n",
    "\n",
    "    c = Classify()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = c.splitToMiniBatches(negativeFeatures, positiveFeatures, indexOfTest)\n",
    "\n",
    "    aveAccuracyNB += c.naiveBayesClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    aveAccuracySVM += c.SVMClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    indexOfTest += 1\n",
    "\n",
    "print(\"Average Accuracy of the Naive Bayes Classifier: \")\n",
    "print(aveAccuracyNB / 3 * 100)\n",
    "print(\"Average Accuracy of the SVM Classifier: \")\n",
    "print(aveAccuracySVM / 3 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cad426ff-cd7a-4f26-841a-5e6c19a57137"
    }
   },
   "source": [
    "### (7) Adjective Unigrams using Presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature is similar to the process in getting the POS tags of the unigrams. But instead of getting all the words, only the adjectives found in the documents will be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 fold in Naive Bayes:  0.754838709677\n",
      "Accuracy for 1 fold in SVM:  0.741935483871\n",
      "Accuracy for 1 fold in Naive Bayes:  0.764516129032\n",
      "Accuracy for 1 fold in SVM:  0.774193548387\n",
      "Accuracy for 1 fold in Naive Bayes:  0.767741935484\n",
      "Accuracy for 1 fold in SVM:  0.729032258065\n",
      "Average Accuracy of the Naive Bayes Classifier: \n",
      "76.2365591398\n",
      "Average Accuracy of the SVM Classifier: \n",
      "74.8387096774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "f = Features()\n",
    "negativeDocuments = f.getListWithNegation(rf.getCorpusNeg())\n",
    "positiveDocuments = f.getListWithNegation(rf.getCorpusPos())\n",
    "\n",
    "kf = KFold(len(rf.getCorpusNeg()))\n",
    "\n",
    "aveAccuracyNB = 0\n",
    "aveAccuracySVM = 0\n",
    "indexOfTest = 0\n",
    "\n",
    "for train, test in kf: #Get training indices and testing indices\n",
    "    negDoc = []\n",
    "    posDoc = []\n",
    "\n",
    "    for index in train:\n",
    "        negDoc.append(negativeDocuments[index])\n",
    "        posDoc.append(positiveDocuments[index])\n",
    "\n",
    "    negativeUnigrams = f.getUnigramAdjective(negDoc)\n",
    "    positiveUnigrams = f.getUnigramAdjective(posDoc)\n",
    "    \n",
    "    negativeFeatures = f.getChosenFeaturesAdjective(negDoc)\n",
    "    positiveFeatures = f.getChosenFeaturesAdjective(posDoc)\n",
    "\n",
    "    f.features['adjectives'] = []\n",
    "\n",
    "    c = Classify()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = c.splitToMiniBatches(negativeFeatures, positiveFeatures, indexOfTest)\n",
    "\n",
    "    aveAccuracyNB += c.naiveBayesClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    aveAccuracySVM += c.SVMClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    indexOfTest += 1\n",
    "\n",
    "print(\"Average Accuracy of the Naive Bayes Classifier: \")\n",
    "print(aveAccuracyNB / 3 * 100)\n",
    "print(\"Average Accuracy of the SVM Classifier: \")\n",
    "print(aveAccuracySVM / 3 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "639f206f-7cf6-4fcb-9584-c988b6584270"
    }
   },
   "source": [
    "### (8) Unigrams + Position of word using Presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also has the process as the unigram but an addition process of determining the word's position was done.\n",
    "\n",
    "First, get the split per quarter of each document. To be able to achieve this, get the total word count of each document and divide this by 4. Words in the 1st quater will be appended with \"-0\". Words falling in the 2nd and 3rd quarter, will be appended with \"-1\". And finally, words in the last quarter will be appended \"-2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getUnigramPosition(self, all_document):\n",
    "        unigramsposition = {}\n",
    "        \n",
    "        for words_in_document, category in all_document:\n",
    "            splitPerQtr = len(words_in_document) // 4\n",
    "            i = 0\n",
    "            position = 0\n",
    "            for word in words_in_document: #START\n",
    "                if i < splitPerQtr:\n",
    "                    unigramsposition[word + \"-\" + str(position)] = unigramsposition.get(word + \"-\" + str(position), 0) + 1\n",
    "                if i >= splitPerQtr and i < len(words_in_document) - splitPerQtr: #MIDDLE\n",
    "                    position = 1\n",
    "                elif i <= len(words_in_document) - splitPerQtr: #END\n",
    "                    position = 2\n",
    "                i += 1 \n",
    "\n",
    "        for word, count in unigramsposition.items():\n",
    "            if count >= 4:\n",
    "                self.features['unigram_position'].append(word)\n",
    "\n",
    "        #print(len(self.features['unigram_position']))\n",
    "        \n",
    "        return self.features['unigram_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 fold in Naive Bayes:  0.612903225806\n",
      "Accuracy for 1 fold in SVM:  0.716129032258\n",
      "Accuracy for 1 fold in Naive Bayes:  0.606451612903\n",
      "Accuracy for 1 fold in SVM:  0.751612903226\n",
      "Accuracy for 1 fold in Naive Bayes:  0.61935483871\n",
      "Accuracy for 1 fold in SVM:  0.722580645161\n",
      "Average Accuracy of the Naive Bayes Classifier: \n",
      "61.2903225806\n",
      "Average Accuracy of the SVM Classifier: \n",
      "73.0107526882\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "f = Features()\n",
    "negativeDocuments = f.getListWithNegation(rf.getCorpusNeg())\n",
    "positiveDocuments = f.getListWithNegation(rf.getCorpusPos())\n",
    "\n",
    "kf = KFold(len(rf.getCorpusNeg()))\n",
    "\n",
    "aveAccuracyNB = 0\n",
    "aveAccuracySVM = 0\n",
    "indexOfTest = 0\n",
    "\n",
    "for train, test in kf: #Get training indices and testing indices\n",
    "    negDoc = []\n",
    "    posDoc = []\n",
    "\n",
    "    for index in train:\n",
    "        negDoc.append(negativeDocuments[index])\n",
    "        posDoc.append(positiveDocuments[index])\n",
    "\n",
    "    negativeUnigrams = f.getUnigramPosition(negDoc)\n",
    "    positiveUnigrams = f.getUnigramPosition(posDoc)\n",
    "    \n",
    "    negativeFeatures = f.getChosenFeaturesPosition(negDoc)\n",
    "    positiveFeatures = f.getChosenFeaturesPosition(posDoc)\n",
    "\n",
    "    f.features['unigram_position'] = []\n",
    "\n",
    "    c = Classify()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = c.splitToMiniBatches(negativeFeatures, positiveFeatures, indexOfTest)\n",
    "\n",
    "    aveAccuracyNB += c.naiveBayesClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    aveAccuracySVM += c.SVMClassifier(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    indexOfTest += 1\n",
    "\n",
    "print(\"Average Accuracy of the Naive Bayes Classifier: \")\n",
    "print(aveAccuracyNB / 3 * 100)\n",
    "print(\"Average Accuracy of the SVM Classifier: \")\n",
    "print(aveAccuracySVM / 3 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results are slightly different from the original. There are several reasons in why this may happen. First, the original authors did not state which negation words and punctuation marks were used. Also, the POS tagger may also affect the result of the features. Some of the pre-process made may also be different "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "class Features(object):\n",
    "    def __init__(self):\n",
    "        self.negate = \"NOT_\"\n",
    "\n",
    "        self.negation_list = [\"no\", \"not\", \"n\\'t\"]\n",
    "        self.punctuation_list = [\".\", \",\", \"?\", \"!\", \";\"]\n",
    "\n",
    "        self.features = {}\n",
    "\n",
    "        self.features['unigram'] = []\n",
    "        self.features['bigram'] = []\n",
    "        self.features['unigram_bigram'] = []\n",
    "        self.features['topunigrams'] = []\n",
    "        self.features['unigram_pos'] = []\n",
    "        self.features['adjectives'] = []\n",
    "        self.features['unigram_position'] = []\n",
    "\n",
    "    def getListWithNegation(self, corpus):\n",
    "        all_document = []\n",
    "\n",
    "        for document, category in corpus:\n",
    "            all_words = []\n",
    "            tokens = nltk.word_tokenize(document)\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if tokens[i] in self.negation_list:\n",
    "                    all_words.append(tokens[i])\n",
    "                    i += 1\n",
    "                    while i < len(tokens):\n",
    "                        if tokens[i] not in self.punctuation_list:\n",
    "                            a = tokens[i] + self.negate\n",
    "                            all_words.append(a)\n",
    "                        else:\n",
    "                            break\n",
    "                        i += 1\n",
    "                else:\n",
    "                    all_words.append(tokens[i])\n",
    "                i += 1\n",
    "\n",
    "            all_document.append((all_words, category))\n",
    "\n",
    "        #print(all_document)\n",
    "\n",
    "        #print(all_words)\n",
    "\n",
    "        return all_document\n",
    "\n",
    "    def getUnigram(self, all_document):\n",
    "        unigrams = {}\n",
    "\n",
    "        for words_in_document, category in all_document:\n",
    "            for word in words_in_document:\n",
    "                unigrams[word] = unigrams.get(word, 0) + 1\n",
    "\n",
    "        for word, count in unigrams.items():\n",
    "            if count >= 4: #SABI SA DOCU AT LEAST 4\n",
    "                self.features['unigram'].append(word)\n",
    "\n",
    "        #print(len(self.features['unigram']))\n",
    "\n",
    "        return self.features['unigram']\n",
    "\n",
    "    def getChosenFeatures(self, all_document, type = 'None'):\n",
    "        chosen_features = np.zeros((len(all_document), len(self.features['unigram'])))\n",
    "\n",
    "        i = 0\n",
    "        for words_in_document, category in all_document:\n",
    "            if type == 'frequency':\n",
    "                frequencies = Counter(words_in_document)\n",
    "                for word, count in frequencies.items():\n",
    "                    try:\n",
    "                        chosen_features[i][self.features['unigram'].index(word)] = count\n",
    "                    except: #PAG WALA\n",
    "                        pass\n",
    "            elif type == 'presence':\n",
    "                for word in set(words_in_document):\n",
    "                    try:\n",
    "                        chosen_features[i][self.features['unigram'].index(word)] = 1\n",
    "                    except: #PAG WALA\n",
    "                        pass\n",
    "            i += 1\n",
    "\n",
    "        #print(chosen_features)\n",
    "\n",
    "        return chosen_features\n",
    "\n",
    "    def getBigram(self, all_document):\n",
    "        bigrams = {}\n",
    "        words = []\n",
    "\n",
    "        for words_in_document, category in all_document:\n",
    "            #tokens = nltk.word_tokenize(words_in_document)\n",
    "            words += ngrams(words_in_document, 2)\n",
    "        \n",
    "        for word in words:\n",
    "            bigrams[word] = bigrams.get(word, 0) + 1\n",
    "\n",
    "        for word, count in bigrams.items():\n",
    "            if count >= 7: #SABI SA DOCU AT LEAST 7\n",
    "                self.features['bigram'].append(word)\n",
    "\n",
    "        #print(self.features['bigram'][:100])\n",
    "        return self.features['bigram'][:16165]\n",
    "\n",
    "    def getChosenFeaturesBigram(self, all_document):\n",
    "        chosen_features = np.zeros((len(all_document), len(self.features['bigram'])))\n",
    "\n",
    "        i = 0\n",
    "        for words_in_document, category in all_document:\n",
    "            words = []\n",
    "            #tokens = nltk.word_tokenize(words_in_document)\n",
    "            words += ngrams(words_in_document, 2)\n",
    "            for word in set(words):\n",
    "                try:\n",
    "                    chosen_features[i][self.features['bigram'].index(word)] = 1\n",
    "                except: #PAG WALA\n",
    "                    pass\n",
    "            i += 1\n",
    "\n",
    "        #print(chosen_features)\n",
    "        return chosen_features\n",
    "\n",
    "    def getUnigramBigram(self):\n",
    "        self.features[unigram_bigram] = self.features['unigram'] + self.features['bigram']\n",
    "    \n",
    "    def getChosenFeaturesUniBigram(self, all_document, features):\n",
    "        chosen_features = np.zeros((len(all_document), len(features)))\n",
    "\n",
    "        i = 0\n",
    "        for words_in_document, category in all_document:\n",
    "            words = []\n",
    "            #tokens = nltk.word_tokenize(words_in_document)\n",
    "            words += ngrams(words_in_document, 2)\n",
    "            for word in set(words):\n",
    "                try:\n",
    "                    chosen_features[i][features.index(word)] = 1\n",
    "                except: #PAG WALA\n",
    "                    pass\n",
    "            i += 1\n",
    "\n",
    "        #print(chosen_features)\n",
    "        return chosen_features\n",
    "\n",
    "    def getTopUnigrams(self, all_document):\n",
    "        frequencies = Counter(self.features['unigram'])\n",
    "        self.features['topunigrams'] = sorted(frequencies, key=frequencies.get, reverse=True)[:2633]\n",
    "\n",
    "        chosen_features = np.zeros((len(all_document), len(self.features['topunigrams'])))\n",
    "\n",
    "        i = 0\n",
    "        for words_in_document, category in all_document:\n",
    "            for word in set(words_in_document):\n",
    "                try:\n",
    "                    chosen_features[i][self.features['topunigrams'].index(word)] = 1\n",
    "                except: #PAG WALA\n",
    "                    pass\n",
    "            i += 1\n",
    "\n",
    "        #print(chosen_features)\n",
    "        return chosen_features\n",
    "\n",
    "    def getUnigramPos(self, all_document):\n",
    "        unigramspos = {}\n",
    "\n",
    "        for words_in_document, category in all_document:\n",
    "            for word, pos in nltk.pos_tag(words_in_document):\n",
    "                unigramspos[word + \"-\" + pos] = unigramspos.get(word + \"-\" + pos, 0) + 1\n",
    "\n",
    "        for word, count in unigramspos.items():\n",
    "            if count >= 4:\n",
    "                self.features['unigram_pos'].append(word)\n",
    "\n",
    "        #print(len(self.features['unigram_pos']))\n",
    "        \n",
    "        return self.features['unigram_pos']\n",
    "\n",
    "    def getChosenFeaturesPos(self, all_document):\n",
    "        chosen_features = np.zeros((len(all_document), len(self.features['unigram_pos'])))\n",
    "\n",
    "        i = 0\n",
    "        for words_in_document, category in all_document:\n",
    "            for word, pos in nltk.pos_tag(words_in_document):\n",
    "                try:\n",
    "                    chosen_features[i][self.features['unigram_pos'].index(word + \"-\" + pos)] = 1\n",
    "                except: #PAG WALA\n",
    "                    pass\n",
    "            i += 1\n",
    "\n",
    "        #print(chosen_features)\n",
    "        return chosen_features\n",
    "\n",
    "    def getUnigramAdjective(self, all_document):\n",
    "        for words_in_document, category in all_document:\n",
    "            for word, pos in nltk.pos_tag(words_in_document):\n",
    "                if pos in ['JJ', 'JJR', 'JJS']:\n",
    "                    self.features['adjectives'].append(word)\n",
    "        \n",
    "        return self.features['adjectives']\n",
    "    \n",
    "    def getChosenFeaturesAdjective(self, all_documents):\n",
    "        chosen_features = np.zeros((len(all_document), len(self.features['adjectives'])))\n",
    "        \n",
    "        i = 0\n",
    "        for words_in_document, category in all_document:\n",
    "            for word in set(words_in_document):\n",
    "                try:\n",
    "                    chosen_features[i][self.features['adjectives'].index(word)] = 1\n",
    "                except: #PAG WALA\n",
    "                    pass\n",
    "            i += 1\n",
    "\n",
    "        #print(chosen_features)\n",
    "        return chosen_features\n",
    "\n",
    "    def getUnigramPosition(self, all_document):\n",
    "        unigramsposition = {}\n",
    "        \n",
    "        for words_in_document, category in all_document:\n",
    "            splitPerQtr = len(words_in_document) // 4\n",
    "            i = 0\n",
    "            position = 0\n",
    "            for word in words_in_document: #START\n",
    "                if i < splitPerQtr:\n",
    "                    unigramsposition[word + \"-\" + str(position)] = unigramsposition.get(word + \"-\" + str(position), 0) + 1\n",
    "                if i >= splitPerQtr and i < len(words_in_document) - splitPerQtr: #MIDDLE\n",
    "                    position = 1\n",
    "                elif i <= len(words_in_document) - splitPerQtr: #END\n",
    "                    position = 2\n",
    "                i += 1 \n",
    "\n",
    "        for word, count in unigramsposition.items():\n",
    "            if count >= 4:\n",
    "                self.features['unigram_position'].append(word)\n",
    "\n",
    "        #print(len(self.features['unigram_position']))\n",
    "        \n",
    "        return self.features['unigram_position']\n",
    "\n",
    "    def getChosenFeaturesPosition(self, all_document):\n",
    "        chosen_features = np.zeros((len(all_document), len(self.features['unigram_position'])))\n",
    "\n",
    "        i = 0\n",
    "        for words_in_document, category in all_document:\n",
    "            splitPerQtr = len(words_in_document) // 4\n",
    "            j = 0\n",
    "            position = 0\n",
    "            for word in set(words_in_document):\n",
    "                try:\n",
    "                    if j < splitPerQtr:\n",
    "                        chosen_features[i][self.features['unigram_position'].index(word + \"-\" + str(position))] = 1\n",
    "                    if j >= splitPerQtr and j < len(words_in_document) - splitPerQtr: #MIDDLE\n",
    "                        position = 1\n",
    "                    elif j <= len(words_in_document) - splitPerQtr: #END\n",
    "                        position = 2 \n",
    "                    j += 1\n",
    "                except: #PAG WALA\n",
    "                    j += 1\n",
    "                    pass\n",
    "            i += 1\n",
    "\n",
    "        #print(chosen_features)\n",
    "        \n",
    "        return chosen_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Classify(object):\n",
    "    def __init__(self):\n",
    "        self.number_fold = 3\n",
    "\n",
    "    def splitToMiniBatches(self, negative, positive, indexOfTest):\n",
    "        batch = {'fold' : [], 'X' : [], 'Y' : []}\n",
    "\n",
    "        split = len(negative) // self.number_fold\n",
    "\n",
    "        #Create mini batches\n",
    "        for i in range(self.number_fold):\n",
    "            mergedlist = []\n",
    "\n",
    "            batch['fold'].append(i)\n",
    "            \n",
    "            #Split to equal sized batch, maintaining balance from neg and pos\n",
    "            neg = negative[i * split : i * split + split]\n",
    "            #print(len(neg))\n",
    "            pos = positive[i * split : i * split + split]\n",
    "            #print(len(pos))\n",
    "            \n",
    "            mergedlist.extend(neg)\n",
    "            mergedlist.extend(pos)\n",
    "            batch['X'].append(mergedlist)\n",
    "            #print(len(self.batch['X'][i]))\n",
    "\n",
    "            batch['Y'].append(np.append(np.zeros(split), np.ones(split)))\n",
    "            #print(len(self.batch['Y'][i]))\n",
    "\n",
    "        batch['fold'] = np.array(batch['fold'])\n",
    "        batch['X'] = np.array(batch['X'])\n",
    "        batch['Y'] = np.array(batch['Y'])\n",
    "\n",
    "        x_train = batch['X'][batch['fold'] != i]\n",
    "        y_train = batch['Y'][batch['fold'] != i]\n",
    "\n",
    "        x_test = batch['X'][i]\n",
    "        y_test = batch['Y'][i]\n",
    "\n",
    "        #print(x_test.shape)\n",
    "        #print(y_test.shape)\n",
    "\n",
    "        return x_train, x_test, y_train, y_test\n",
    "    \n",
    "    def naiveBayesClassifier(self, x_train, x_test, y_train, y_test):\n",
    "        bernoulliNB = BernoulliNB()\n",
    "        bernoulliNB.fit(x_train.reshape(x_train.shape[0] * x_train.shape[1], -1), np.ravel(y_train.reshape(y_train.shape[0] * y_train.shape[1], -1)))\n",
    "        accuracy = accuracy_score(bernoulliNB.predict(x_test), y_test)\n",
    "        \n",
    "        #print(\"Accuracy for 1 fold in Naive Bayes: \", accuracy)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def SVMClassifier(self, x_train, x_test, y_train, y_test):\n",
    "        linearSVM = LinearSVC()\n",
    "        linearSVM.fit(x_train.reshape(x_train.shape[0] * x_train.shape[1], -1), np.ravel(y_train.reshape(y_train.shape[0] * y_train.shape[1], -1)))\n",
    "        accuracy = accuracy_score(linearSVM.predict(x_test), y_test)\n",
    "    \n",
    "        #print(\"Accuracy for 1 fold in SVM: \", accuracy)\n",
    "\n",
    "        return accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
